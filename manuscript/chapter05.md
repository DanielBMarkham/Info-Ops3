One of the more interesting aspects of negatives stacking and the importance of being wrong is how if affects all sorts of political interactions. As it turns out, quite surprisingly, people complaining, even in extremely vague language, are the bedrock of all progress of western civilzation.

It's quite easy to take an instance of somebody complaining in vague terms, perhaps somebody saying "this sux!", and point out how completely useless that kind of comment is. And that's correct! All negative comments turn out to be useless. It is the comment itself that begins the process of analysis that then leads to something productive. Once we narrow down things to the point where it doesn't matter whether we act, then and only then do we add a test.

That test is made using the same language of negatives that we have stacked to find a particular area of interest that we're currently exploring. 

This does not guarantee success. Or that we're right. But it guarantees that we will have the same experience most all of the time. Science, as it turns out, relies on negatives stacking, tests corresponding to that uniquely-experienced area of inquiry (using the same language as the area it's described in), and reproducible results. That's it.

We'd like to think of science as being the progress of humans stacked on top of one another over the ages. One writer described it as "standing on the shoulders of giants". And that's true. But it's not true in a literal way. We are not literally standing on the shoulders of giants, nor are we using the things we've written down over the years to do that, as much as we'd like to think otherwise. We are not taking the words of previous people and adding our own words to them to make something new. 

That's probably the most surprising thing of all, out of all of this. 

What we're doing is creating areas where a small group of people have an interest in exploring, or silos, where we have agreed-upon negatives that stack to create testable, reproducible, experiments that correspond to the area we're trying to explore or the thing we're trying to find. Since those negativts are by definitions unprecise and vague, that language does not have to resolve itself into some sort of formal, logical, self-consistent graph of meaning. So while we can certainly use vague language to describe whatever our target is, and we can use vague language to describe how we reproduce things, the corresponding part of this, the silo'ed negatives stacking part? That is a socially-constructed boundary of vagaries. And that's fine. It seems to work quite well for everybody concerned.

There was a time when the human language was small enough and the concepts generally understood in a shared way well enough, that you could read a book and then go do the thing while understanding the entire "stack" of tech you're using to accomplish that. Those days are long gone, but we still have the illusion that they're not, mainly because of the limits of human cognition. 

This is why it's possible to describe lots of experiments that seem like they might work to experts in the field when in fact they're not reproducible or the experiments are reproducible but the results aren't. It's not a problem with the experts or the field of study.

Popper got very close to this when he said that all science should be falsifiable. It's not that it should be falsifiable as much as it should contain self-consistent vague language that describes a reproducible, falsifiable system of inquiry. 

Dang, that's a mouthful of words, isn't it?

So the beginning of the learning loop, by definition, is a vague, formless, negative feeling. This feeling is based either on sensory input, some internal caluculation, or a sequence of either of the two in combination. But make no mistake, it is that beginning, it is that that vague sense of negativeness, where learning begins.

We cannot natively, naively, or intuitively see this. The human brain is a cheater. Describing how learning happens, like we just did, is a mess of words and complexities. When we're diving down into this particular pool, it's quite easy to think either the entire concept is very fuzzy and we'll never, ever reach our goals of understanding learning at scale. Brain don't care about any of this. 

Biological systems work on this same learning principle. But they don't care about the rigor of reproducibility, vagueness, falsifibility, or negatives stacking to create boundaries as much as science does. (Even if they do in practice). That's because biological systems only have to believe that they understand what's going on and that there's a cause and effect in place. Scientific systems have to **know** and be able to describe all of that. 

The word "know" here is also quite interesting. Once we dive down microscopically to the quantum level? Cause and effect? Locality? Particles? Uniqueness? Even the idea of space and time? They become amorphous. 

Indeed, there are a lot of philosophers of science that are either frequentists or bayesians. By that I mean that they do not view science as a lock-step, cause-and-effect area of exploration as much as a "set this up, do this thing, and mostly this other thing will happen" situation. Sometimes, like dropping a hammer on Earth, the word "mostly" corresponds to 100%. Sometimes, like at the quantum level, the word "mostly" corresponds to a probably field, matrices of values with probabilities corresponding in multiple dimensions. 

But you know? No matter what physical reality is, brain don't care. If it did care? If we had to evolve as a biological species the same way as a computer program has to be coded? We never would have evolved at all. It's important to understand that while the fundamentals of learning for both biological, machine, and social systems, the actual way that learning takes place varies drastically between all three. 

What we'll end up doing, now that we've established our foundation, is taking these fundamentals and comparing the with the observed situation in all three systems to see where we can create, improve, and measure learning at scale.


Next up, how biological systems can be super-intelligent without actually knowing anything.